{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning with MobileNetV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from train_utils.augmenters import data_augmenter, data_augmenter_fruit360\n",
    "from train_utils.callbacks import LossHistory, LRCallBack\n",
    "from train_utils.utils import is_in\n",
    "from train_utils.losses import crossentropy_loss\n",
    "from train_utils.dataset import food_dataset\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 2 - Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 195823 files belonging to 296 classes.\n",
      "Using 193865 files for training.\n",
      "Found 195823 files belonging to 296 classes.\n",
      "Using 1958 files for validation.\n",
      "Found 89089 files belonging to 96 classes.\n",
      "tf.Tensor(6059, shape=(), dtype=int64)\n",
      "<BatchDataset shapes: ((None, 224, 224, 3), (None, 296)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "train_dataset, cv_dataset = food_dataset(BATCH_SIZE, IMG_SIZE)\n",
    "\n",
    "from train_utils.dataset import fruit360_classes, all_class_names\n",
    "\n",
    "print(\n",
    "    tf.data.experimental.cardinality(train_dataset),\n",
    "    train_dataset,\n",
    "    sep = '\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(all_class_names[tf.math.argmax(labels[i])])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 3 - Preprocess and Augment Training Data\n",
    "(https://www.tensorflow.org/tutorials/images/data_augmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=128, reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_augmentation = data_augmenter_fruit360()\n",
    "\n",
    "for image, _ in train_dataset.take(1):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    first_image = image[1]\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "        plt.imshow(augmented_image[0] / 255)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 4 - Using MobileNetV3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "\n",
    "preprocess_input = tf.keras.applications.mobilenet_v3.preprocess_input\n",
    "\n",
    "mobile_v3_model = tf.keras.applications.MobileNetV3Large(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the last 2 layers here. They are the so called top layers, and they are responsible of the classification in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in the base model:  276\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of layers in the base model: \", len(mobile_v3_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3) (32, 296)\n"
     ]
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(train_dataset))\n",
    "print(image_batch.shape, label_batch.shape)\n",
    "NUM_CLASSES = label_batch.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-2'></a>\n",
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-106ac76f39286ee3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    food_augmenter = data_augmenter()\n",
    "    fruit_augmenter = data_augmenter_fruit360()\n",
    "    fruit_indices = tf.constant(\n",
    "        [all_class_names.index(fruit) for fruit in fruit360_classes]\n",
    "    )\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        class_indices = tf.math.argmax(y, -1)\n",
    "        isfr = is_in(class_indices, self.fruit_indices)\n",
    "        isfr = tf.expand_dims(tf.expand_dims(tf.expand_dims(isfr, -1), -1), -1)\n",
    "        x = tf.where(\n",
    "            isfr, \n",
    "            self.fruit_augmenter(x),\n",
    "            self.food_augmenter(x),\n",
    "        )\n",
    "        return super().train_step((x, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 0 to skip\n",
    "FIXED_LAYERS_V3 = 256    # [0, 276]\n",
    "DROPOUT_BEFORE = 0       # [0, 0.5]\n",
    "TOP_DENSE_LAYER = 0    # (300, 1280) U {0}\n",
    "DROPOUT_AFTER = 0        # [0, 0.5]\n",
    "L1, L2 = 0, 0            # [1e-9, 1e-4] U {0}\n",
    "GRADIENT_CLIP = 100       # [5, 100]\n",
    "\n",
    "LR = (-4, -6)            # [-5.5, -2.5]\n",
    "EPOCHS = 10              # [0, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def food_model(image_shape=IMG_SIZE):\n",
    "    input_shape = image_shape + (3,)\n",
    "    \n",
    "    base_model = tf.keras.applications.MobileNetV3Large(\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "    )\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # Fine-tune from this layer onwards\n",
    "    fine_tune_at = FIXED_LAYERS_V3\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    reg = tf.keras.regularizers.L1L2(L1, L2)\n",
    "        \n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = preprocess_input(inputs)\n",
    "    x = base_model(x)\n",
    "    x = tfl.GlobalAveragePooling2D()(x)\n",
    "    if DROPOUT_BEFORE:\n",
    "        x = tf.keras.layers.Dropout(DROPOUT_BEFORE)(x)\n",
    "    if TOP_DENSE_LAYER:\n",
    "        x = tfl.Dense(\n",
    "            TOP_DENSE_LAYER,\n",
    "            activation='relu', \n",
    "            kernel_regularizer=reg\n",
    "        )(x)\n",
    "    if DROPOUT_AFTER:\n",
    "        x = tf.keras.layers.Dropout(DROPOUT_AFTER)(x)\n",
    "    outputs = tfl.Dense(\n",
    "        NUM_CLASSES, \n",
    "        activation='softmax', \n",
    "        kernel_regularizer=reg\n",
    "    )(x)\n",
    "    \n",
    "    model = CustomModel(inputs, outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate optimal learning rate"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "history = LossHistory(batches=200, l_r=(-5, -0.5))\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=GRADIENT_CLIP)\n",
    "\n",
    "model2 = food_model(IMG_SIZE)\n",
    "model2.compile(\n",
    "    loss=crossentropy_loss,\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "model2.fit(\n",
    "    train_dataset.take(history.batches),\n",
    "    callbacks=[history],\n",
    ")\n",
    "    \n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(history.exp_lrs, history.losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Learning Rate Exponent')\n",
    "plt.title('Training Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"custom_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "MobilenetV3large (Functional (None, 7, 7, 1280)        4226432   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 296)               379176    \n",
      "=================================================================\n",
      "Total params: 4,605,608\n",
      "Trainable params: 2,150,056\n",
      "Non-trainable params: 2,455,552\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(clipvalue=GRADIENT_CLIP)\n",
    "\n",
    "metrics = [\n",
    "    tf.keras.metrics.CategoricalAccuracy(name='top1'),\n",
    "    tf.keras.metrics.TopKCategoricalAccuracy(k=4, name='top4'),\n",
    "]\n",
    "model2 = food_model(IMG_SIZE)\n",
    "model2.compile(\n",
    "    loss=crossentropy_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=metrics,\n",
    ")\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6c8318333545c69d3e88bd39eba650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|                                             0/10 ETA: ?s,  ?epochs/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4743a5946dea40358401b40560d73269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0/6059 ; ETA: ?s                                                                    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr set to: 0.0001\n"
     ]
    }
   ],
   "source": [
    "tqdm_callback = tfa.callbacks.TQDMProgressBar(\n",
    "    metrics_separator=' ; ',\n",
    "    epoch_bar_format='{n_fmt}/{total_fmt} ; ETA: {remaining}s {bar} {desc}',\n",
    "    metrics_format='{name}: {value:0.4f}',\n",
    "    update_per_second=1,\n",
    ")\n",
    "lr_callback = LRCallBack(epochs=EPOCHS, l_r=LR)\n",
    "\n",
    "history = model2.fit(\n",
    "    train_dataset,\n",
    "    validation_data=cv_dataset,\n",
    "    epochs=lr_callback.epochs,\n",
    "    verbose=0,\n",
    "    callbacks=[tqdm_callback, lr_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plt.plot(lr_callback.batch_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Batches')\n",
    "plt.title('Training Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "acc = history.history['top1']\n",
    "val_acc = history.history['val_top1']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,0.01])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Network Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(cv_dataset))\n",
    "plt.figure(figsize=(25, 25))\n",
    "for item in range(9):\n",
    "    ax = plt.subplot(3, 3, item + 1)\n",
    "    plt.imshow(image_batch[item].numpy().astype(\"uint8\"))\n",
    "    probs = model2(image_batch)[item]\n",
    "    label = all_class_names[tf.math.argmax(label_batch[item])]\n",
    "    pred = all_class_names[tf.math.argmax(probs)]\n",
    "    plt.title(label + '->' + pred)\n",
    "    plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
